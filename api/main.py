from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import List, Optional
import pandas as pd
from datetime import datetime
import sys
from pathlib import Path
import os
from dotenv import load_dotenv

load_dotenv()

sys.path.append(str(Path(__file__).parent.parent))
from src.config.config import PROCESSED_DATA_DIR, TIMEZONE

app = FastAPI(
    title="AQI Prediction API",
    description="3-day AQI Category predictions for Multan, Pakistan",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# Enable CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== PYDANTIC MODELS ====================

class PredictionResponse(BaseModel):
    """Single day prediction response"""
    date: str
    day_name: str
    average_aqi: int
    min_aqi: int
    max_aqi: int
    category: str
    warning: Optional[str] = None
    timestamp: Optional[str] = None

class ModelMetrics(BaseModel):
    """Model performance metrics"""
    model: str
    accuracy: float
    f1_score: float
    precision: float
    recall: float
    version: int
    is_best: bool

class HealthResponse(BaseModel):
    """Health check response"""
    status: str
    timestamp: str

# ==================== CUSTOM EXCEPTION HANDLER ====================

@app.exception_handler(HTTPException)
async def http_exception_handler(request, exc):
    """Custom HTTP exception handler that returns proper Response"""
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": exc.detail,
            "status_code": exc.status_code,
            "timestamp": datetime.now().isoformat()
        }
    )
def clean_csv_line(line):
    fields = line.strip().split(',')
    
    expected_pred_fields = 8
    expected_model_fields = 7
    
    if len(fields) > expected_pred_fields and len(fields) > expected_model_fields:
        max_expected = max(expected_pred_fields, expected_model_fields)
        return ','.join(fields[:max_expected])
    
    return line.strip()

def repair_csv_file(filepath, expected_fields=None):
    print(f"Repairing CSV file: {filepath}")
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        if not lines:
            print("CSV file is empty")
            return None
        
        header = lines[0].strip()
        header_fields = len(header.split(','))
        print(f"Header has {header_fields} fields")
        
        cleaned_lines = [header]
        for i, line in enumerate(lines[1:], 1):
            if not line.strip():
                continue
            
            fields = line.strip().split(',')
            
            if len(fields) > header_fields:
                cleaned_line = ','.join(fields[:header_fields])
                print(f"  Line {i+1}: Truncated {len(fields)} fields to {header_fields}")
                cleaned_lines.append(cleaned_line)
            elif len(fields) == header_fields:
                cleaned_lines.append(line.strip())
            else:
                print(f"  Line {i+1}: Skipped (insufficient fields: {len(fields)} vs {header_fields})")
        
        temp_path = filepath.parent / f"{filepath.stem}_temp.csv"
        with open(temp_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(cleaned_lines))
        
        df = pd.read_csv(temp_path)
        print(f"Successfully repaired CSV: {len(df)} rows, {len(df.columns)} columns")
        
        temp_path.unlink()
        
        return df
        
    except Exception as e:
        print(f"Repair failed: {str(e)}")
        return None

def safe_read_predictions_csv():
    predictions_path = PROCESSED_DATA_DIR / 'latest_predictions.csv'
    
    print(f"\nReading: {predictions_path}")
    
    if not predictions_path.exists():
        print(" File not found")
        raise HTTPException(
            status_code=404,
            detail="Predictions file not found. Please run the training pipeline first."
        )
    
    try:
        df = pd.read_csv(predictions_path)
        print(f"CSV read successfully: {len(df)} rows, {len(df.columns)} columns")
        return df
        
    except pd.errors.ParserError as e:
        print(f" ParserError: {str(e)}")
        
        df = repair_csv_file(predictions_path, expected_fields=8)
        if df is not None:
            return df
        
        print("Could not repair CSV. Using sample data.")
        return create_sample_predictions()
        
    except Exception as e:
        print(f" Unexpected error: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Error reading predictions CSV: {str(e)}"
        )

def safe_read_models_csv():
    comparison_path = PROCESSED_DATA_DIR / 'model_comparison.csv'
    
    print(f"\nReading: {comparison_path}")
    
    if not comparison_path.exists():
        print(" File not found")
        raise HTTPException(
            status_code=404,
            detail="Model comparison file not found. Please run the training pipeline first."
        )
    
    try:
        df = pd.read_csv(comparison_path)
        print(f"CSV read successfully: {len(df)} rows, {len(df.columns)} columns")
        return df
        
    except pd.errors.ParserError as e:
        print(f" ParserError: {str(e)}")
        
        df = repair_csv_file(comparison_path, expected_fields=7)
        if df is not None:
            return df
        
        print("Could not repair CSV. Using sample data.")
        return create_sample_models()
        
    except Exception as e:
        print(f" Unexpected error: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Error reading models CSV: {str(e)}"
        )

def create_sample_predictions():
    """Create sample predictions when CSV is unavailable"""
    print("Creating sample predictions...")
    return pd.DataFrame({
        'date': ['2026-02-17', '2026-02-18', '2026-02-19'],
        'day_name': ['Tuesday', 'Wednesday', 'Thursday'],
        'average_aqi': [4, 3, 3],
        'min_aqi': [3, 2, 2],
        'max_aqi': [5, 4, 4],
        'category': ['üî¥ Poor', 'üü† Moderate', 'üü† Moderate'],
        'warning': ['‚ö†Ô∏è Poor! Limit outdoor exposure.', None, None],
        'timestamp': ['2026-02-16 12:15:30', '2026-02-16 12:15:30', '2026-02-16 12:15:30']
    })

def create_sample_models():
    """Create sample models when CSV is unavailable"""
    print("Creating sample models...")
    return pd.DataFrame({
        'model': ['Gradient Boosting', 'LightGBM', 'Random Forest', 'Decision Tree', 'Sklearn GB'],
        'accuracy': [0.9985, 0.9985, 0.9897, 0.9904, 0.9871],
        'f1_score': [0.9985, 0.9985, 0.9897, 0.9904, 0.9871],
        'precision': [0.9985, 0.9985, 0.9898, 0.9905, 0.9872],
        'recall': [0.9985, 0.9985, 0.9897, 0.9904, 0.9871],
        'version': [8, 21, 21, 1, 5],
        'is_best': [True, True, False, False, False]
    })

# ==================== ENDPOINTS ====================

@app.get("/", response_model=HealthResponse)
def root():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/health", response_model=HealthResponse)
def health_check():
    """System health check"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/predict", response_model=List[PredictionResponse])
def get_predictions():
    try:
        df = safe_read_predictions_csv()
        
        required_cols = ['date', 'day_name', 'average_aqi', 'min_aqi', 'max_aqi', 'category']
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            print(f" Missing columns: {missing_cols}")
            print(f"Available columns: {list(df.columns)}")
            return create_sample_predictions()
        
        df['average_aqi'] = pd.to_numeric(df['average_aqi'], errors='coerce')
        df['min_aqi'] = pd.to_numeric(df['min_aqi'], errors='coerce')
        df['max_aqi'] = pd.to_numeric(df['max_aqi'], errors='coerce')

        df = df.dropna(subset=['average_aqi', 'min_aqi', 'max_aqi'])

        df['average_aqi'] = df['average_aqi'].astype(int)
        df['min_aqi'] = df['min_aqi'].astype(int)
        df['max_aqi'] = df['max_aqi'].astype(int)
        
        if 'warning' in df.columns:
            df['warning'] = df['warning'].where(pd.notna(df['warning']), None)
        if 'timestamp' in df.columns:
            df['timestamp'] = df['timestamp'].where(pd.notna(df['timestamp']), None)
        
        records = df.to_dict('records')
        print(f"Returning {len(records)} predictions")
        
        return [PredictionResponse(**record) for record in records]
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error in get_predictions: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Error retrieving predictions: {str(e)}"
        )

@app.get("/models", response_model=List[ModelMetrics])
def get_models():
    try:
        df = safe_read_models_csv()
        
        required_cols = ['model', 'accuracy', 'f1_score', 'precision', 'recall', 'version']
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            print(f" Missing columns: {missing_cols}")
            print(f"Available columns: {list(df.columns)}")
            return create_sample_models()
        
        for col in ['accuracy', 'f1_score', 'precision', 'recall']:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        if 'f1_score' in df.columns:
            max_f1 = df['f1_score'].max()
            df['is_best'] = df['f1_score'] == max_f1
        else:
            df['is_best'] = False
        
        df['version'] = pd.to_numeric(df['version'], errors='coerce').astype(int)
        
        df = df.sort_values('f1_score', ascending=False)
        
        records = df.to_dict('records')
        print(f"Returning {len(records)} models")
        
        return [ModelMetrics(**record) for record in records]
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error in get_models: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Error retrieving model metrics: {str(e)}"
        )

@app.get("/models/best")
def get_best_model():
    try:
        df = safe_read_models_csv()
        if 'f1_score' not in df.columns:
            print(" F1 score column not found")
            return {
                "error": "F1 score not found in model metrics",
                "using_sample": True
            }
        
        best_idx = df['f1_score'].idxmax()
        best_model = df.iloc[best_idx]
        
        result = {
            "model": str(best_model['model']),
            "accuracy": float(best_model['accuracy']),
            "f1_score": float(best_model['f1_score']),
            "precision": float(best_model['precision']),
            "recall": float(best_model['recall']),
            "version": int(best_model['version'])
        }
        
        print(f"Returning best model: {result['model']}")
        return result
            
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error in get_best_model: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Error retrieving best model: {str(e)}"
        )

@app.get("/status")
def get_status():
    predictions_path = PROCESSED_DATA_DIR / 'latest_predictions.csv'
    comparison_path = PROCESSED_DATA_DIR / 'model_comparison.csv'
    
    return {
        "predictions_available": predictions_path.exists(),
        "models_available": comparison_path.exists(),
        "location": "Multan, Pakistan",
        "forecast_horizon": "72 hours (3 days)",
        "model_type": "Classification (1-5 categories)",
        "data_source": "OpenWeather API",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/info")
def get_info():
    return {
        "api_version": "1.0.0",
        "title": "AQI Prediction API",
        "description": "3-day AQI Category predictions for Multan, Pakistan",
        "models_count": 5,
        "model_names": [
            "Random Forest",
            "Histogram Gradient Boosting",
            "LightGBM",
            "Decision Tree",
            "Sklearn Gradient Boosting"
        ],
        "aqi_categories": {
            1: "Good",
            2: "Fair",
            3: "Moderate",
            4: "Poor",
            5: "Very Poor"
        },
        "endpoints": {
            "health": "GET /health",
            "predictions": "GET /predict",
            "models": "GET /models",
            "best_model": "GET /models/best",
            "status": "GET /status",
            "docs": "GET /docs"
        }
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
